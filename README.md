# Telegram Bot на базе языковой модели Llama

Данный проект представляет собой Telegram-бота, который использует локальную языковую модель (LLM) формата GGUF через библиотеку `llama-cpp-python` для генерации ответов. Бот поддерживает контекстные диалоги, позволяет пользователям настраивать отображаемые имена (своё и имя бота), а также отслеживать использование токенов и управлять историей общения.

## Возможности

- **Диалог с LLM**: каждое сообщение пользователя обрабатывается моделью, которая генерирует ответ с учётом предыстории.
- **Управление контекстом**: история диалога автоматически обрезается, если число токенов превышает допустимый лимит модели.
- **Настройка имён**: функции позволяют изменить, как бот обращается к пользователю и как себя называет.
- **Статистика токенов**: функционал показывает, сколько токенов занято в текущем контексте.
- **Сброс истории**: функционал удаляет всю историю диалога, оставляя только начальный промпт.

## Как это работает

Проект состоит из двух основных файлов:

- **logic.py** — содержит функции для работы с данными пользователей, историей диалогов, логированием, а также инициализирует модель Llama.
- **bot.py** — реализует Telegram-бота на библиотеке `aiogram`, обрабатывает команды и текстовые сообщения, вызывает функции из `logic.py` для генерации ответов и управления состоянием.

### Управление контекстом

При добавлении нового сообщения, функция `trim_history` проверяет, не превышает ли текущая история лимит токенов модели (с запасом 256 токенов). Если превышает, она удаляет самые старые сообщения (кроме стартового промпта), пока история не поместится в лимит.

## Примечания

- Перед использованием необходимо указать путь к файлу модели GGUF и путь к файлу стартового промпта в `logic.py`.
- Токен Telegram-бота задаётся в файле `config.py` (не включён в репозиторий).
- Стартовый промпт загружается из текстового файла и используется как основа для задания поведения модели.
